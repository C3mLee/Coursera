{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into postgres\n",
    "\n",
    "1. Create table and set encodings manually (can be extracted from the given target_car.txt file by copying upto ALTER command)\n",
    "2. Remove everything which is above COPY command in target_car.txt file\n",
    "3. run it in command prompt (make sure to set environment variable for psql) \n",
    "   psql -h localhost -d postgres -U postgres < \"Path_to_the_file\\target_car.txt\"\n",
    "4. Run the following script one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data\n",
    "\n",
    "Removed extra columns which can be extrapolated from the given information and exported to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Normalized Supplier Data\n"
     ]
    }
   ],
   "source": [
    "# After clearing everything till COPY:\n",
    "# psql -h localhost -d postgres -U postgres < \"D:\\AndroidUdacity\\Data Analysis\\target_car.txt\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import psycopg2 as pg\n",
    "\n",
    "FILE_PATH = 'D:\\\\AndroidUdacity\\\\Data Analysis\\\\'\n",
    "with open(FILE_PATH+'supplier_car.json') as f:\n",
    "    complete_json_data = []\n",
    "    for line in f:\n",
    "        j_content = json.loads(line)\n",
    "        del j_content['TypeNameFull']\n",
    "        del j_content['TypeName']\n",
    "        complete_json_data.append(j_content)\n",
    "        \n",
    "df.to_excel(FILE_PATH+'normalized_data.xls', index=False)\n",
    "print('Step 1: Normalized Supplier Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data\n",
    "\n",
    "Converted the given attributes to columns and kept the relevant ones which are in accordance with target_car dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Extracted relevant data from the normalized supplier data\n"
     ]
    }
   ],
   "source": [
    "mileage = []\n",
    "mileage_unit = []\n",
    "df = pd.read_excel(FILE_PATH+'normalized_data.xls', sheet_name=0) # can also index sheet by name or fetch all sheets\n",
    "df = df.pivot_table(values='Attribute Values', index=['ID', 'MakeText', 'ModelText','ModelTypeText'], columns='Attribute Names'\n",
    ", aggfunc='first')\n",
    "df = df[[ 'BodyColorText', 'BodyTypeText', \n",
    "         'City', 'ConditionTypeText', 'ConsumptionTotalText', 'DriveTypeText',\n",
    "         'FirstRegMonth', 'FirstRegYear' ]]\n",
    "\n",
    "for col in df['ConsumptionTotalText'].values:\n",
    "        if (str(col) == 'nan'):\n",
    "            mileage.append(' ')\n",
    "            mileage_unit.append(' ')\n",
    "        else:\n",
    "            mileage.append(col[0:-2])\n",
    "            mileage_unit.append(col[-2:])\n",
    "del df['ConsumptionTotalText']\n",
    "df['mileage'], df['mileage_unit'] = mileage, mileage_unit\n",
    "df.to_excel(FILE_PATH+'Extracted_data.xls')\n",
    "print('Step 2: Extracted relevant data from the normalized supplier data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Supplier data\n",
    "\n",
    "1. Renaming columns to match with the current schema so that both the datasets can be integrated.\n",
    "2. Executing PostgreSQL commands to insert the data into the database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(FILE_PATH+'Extracted_data.xls', sheet_name=0) # can also index sheet by name or fetch all sheets\n",
    "  \n",
    "df.rename(columns = {\n",
    "                   'MakeText': 'make', \n",
    "                   'ModelText': 'model',\n",
    "                   'ModelTypeText': 'model_variant',\n",
    "                   'BodyColorText': 'color',\n",
    "                   'BodyTypeText': 'carType', \n",
    "                   'City': 'city', \n",
    "                   'ConditionTypeText': 'condition', \n",
    "                   'DriveTypeText': 'drive',\n",
    "                   'FirstRegMonth': 'manufacture_month', \n",
    "                   'FirstRegYear': 'manufacture_year'}, \n",
    "                                 inplace = True) \n",
    "df = df.drop(df.columns[[0]], axis=1)\n",
    "df.to_csv(FILE_PATH+'Extracted_data.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host='localhost' port='5432' dbname='postgres' user='postgres' password='welcome,'\")\n",
    "conn.set_client_encoding('UTF8')\n",
    "cur = conn.cursor()\n",
    "f = open(FILE_PATH + 'Extracted_data.csv', 'r')\n",
    "sql = '''target_car(make, model, model_variant, color, \"carType\", city, condition, drive, manufacture_month, manufacture_year, mileage, mileage_unit)'''\n",
    "cur.copy_from(f, sql, sep=',')\n",
    "conn.commit()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Excel sheet for target data\n",
    "\n",
    "Running PostgreSQL commands to fetch and save the data in an Excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated Excel Sheet Generated!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "output_file = FILE_PATH + 'Integrated Data.xls'\n",
    "try:\n",
    "    conn = psycopg2.connect(\"host='localhost' port='5432' dbname='postgres' user='postgres' password='welcome,'\")\n",
    "except:\n",
    "    print(\"Connection failed\")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "try:\n",
    "    cur.execute('''select * from target_car''')\n",
    "except:\n",
    "    print('Enable to execute query')\n",
    "rows=cur.fetchall()\n",
    "try:\n",
    "    import xlwt\n",
    "except ImportError: \"import of xlwt module failed\"\n",
    "\n",
    "# Make spreadsheet\n",
    "workbook = xlwt.Workbook()\n",
    "worksheet = workbook.add_sheet(os.path.split(output_file)[1])\n",
    "\n",
    "worksheet.set_panes_frozen(True)\n",
    "worksheet.set_horz_split_pos(0)\n",
    "worksheet.set_remove_splits(True)\n",
    "\n",
    "# Write rows\n",
    "for colidx,heading in enumerate(cur.description):\n",
    "    worksheet.write(0,colidx,heading[0]) # first element of each tuple\n",
    "\n",
    "# Write rows\n",
    "for rowidx, row in enumerate(rows):\n",
    "    if rowidx != 0:\n",
    "        for colindex, col in enumerate(row):\n",
    "            worksheet.write(rowidx+1, colindex, col) # increment `rowidx` by 1\n",
    "\n",
    "# All done\n",
    "workbook.save(output_file)\n",
    "#print\"finished\"\n",
    "\n",
    "\n",
    "print('Step 3: Integrated Excel Sheet Generated!')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
